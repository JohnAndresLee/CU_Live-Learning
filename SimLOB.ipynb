{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{20231201, 20231204, 20231205, 20231206, 20231207, 20231208, 20231211, 20231212, 20231213, 20231214, 20230703, 20230704, 20230705, 20230706, 20230707, 20231215, 20231218, 20230710, 20230711, 20230712, 20230201, 20230202, 20230203, 20230713, 20230714, 20230206, 20230207, 20230208, 20230209, 20230210, 20230717, 20230718, 20230213, 20230214, 20230215, 20230216, 20230217, 20230721, 20230724, 20230220, 20230221, 20230222, 20230223, 20230224, 20230728, 20230731, 20230227, 20230228, 20231117, 20231219, 20231220, 20231221, 20230801, 20230802, 20230803, 20230804, 20230807, 20230808, 20230809, 20230810, 20230811, 20231120, 20230301, 20230302, 20230303, 20230814, 20230815, 20230306, 20230307, 20230308, 20230309, 20230310, 20230816, 20230817, 20230313, 20230314, 20230315, 20230316, 20230317, 20230821, 20230822, 20230320, 20230321, 20230322, 20230323, 20230324, 20230828, 20230829, 20230327, 20230328, 20230329, 20230330, 20230331, 20231020, 20231121, 20231122, 20231123, 20230818, 20231225, 20231124, 20231226, 20230921, 20231227, 20230922, 20230719, 20231228, 20231025, 20231127, 20230720, 20231229, 20231026, 20231128, 20230823, 20231027, 20231129, 20230824, 20230621, 20231130, 20230825, 20230927, 20230928, 20230725, 20230726, 20230901, 20230727, 20230904, 20230905, 20230906, 20230907, 20230830, 20230908, 20230911, 20230912, 20230831, 20230913, 20230403, 20230404, 20230914, 20230406, 20230407, 20230915, 20230918, 20230410, 20230411, 20230412, 20230413, 20230414, 20230919, 20230920, 20230417, 20230418, 20230419, 20230420, 20230421, 20230925, 20230926, 20230424, 20230425, 20230426, 20230427, 20230428, 20231009, 20231010, 20231011, 20231012, 20231013, 20230504, 20230505, 20231016, 20231017, 20230508, 20230509, 20230510, 20230511, 20230512, 20231018, 20231019, 20230515, 20230516, 20230517, 20230518, 20230519, 20231023, 20231024, 20230522, 20230523, 20230524, 20230525, 20230526, 20231030, 20231031, 20230529, 20230530, 20230531, 20231222, 20231101, 20231102, 20231103, 20231106, 20231107, 20231108, 20231109, 20231110, 20230601, 20230602, 20231113, 20231114, 20230605, 20230606, 20230607, 20230608, 20230609, 20231115, 20231116, 20230612, 20230613, 20230614, 20230103, 20230104, 20230105, 20230106, 20230615, 20230616, 20230109, 20230110, 20230111, 20230112, 20230113, 20230619, 20230620, 20230116, 20230117, 20230118, 20230119, 20230120, 20230626, 20230627, 20230628, 20230629, 20230630, 20230130, 20230131}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TradingDay</th>\n",
       "      <th>LastPrice</th>\n",
       "      <th>PreSettlementPrice</th>\n",
       "      <th>PreClosePrice</th>\n",
       "      <th>PreOpenInterest</th>\n",
       "      <th>OpenPrice</th>\n",
       "      <th>HighestPrice</th>\n",
       "      <th>LowestPrice</th>\n",
       "      <th>OpenInterest</th>\n",
       "      <th>BidPrice1</th>\n",
       "      <th>...</th>\n",
       "      <th>BidPrice4</th>\n",
       "      <th>BidVolume4</th>\n",
       "      <th>AskPrice4</th>\n",
       "      <th>AskVolume4</th>\n",
       "      <th>BidPrice5</th>\n",
       "      <th>BidVolume5</th>\n",
       "      <th>AskPrice5</th>\n",
       "      <th>AskVolume5</th>\n",
       "      <th>delta_Volume</th>\n",
       "      <th>delta_Turnover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-03 08:59:00.500</th>\n",
       "      <td>20230103</td>\n",
       "      <td>66160</td>\n",
       "      <td>66120</td>\n",
       "      <td>66260</td>\n",
       "      <td>99641</td>\n",
       "      <td>66160</td>\n",
       "      <td>66160</td>\n",
       "      <td>66160</td>\n",
       "      <td>99658</td>\n",
       "      <td>66160</td>\n",
       "      <td>...</td>\n",
       "      <td>66130</td>\n",
       "      <td>200</td>\n",
       "      <td>66260</td>\n",
       "      <td>110</td>\n",
       "      <td>66120</td>\n",
       "      <td>13</td>\n",
       "      <td>66270</td>\n",
       "      <td>30</td>\n",
       "      <td>224.0</td>\n",
       "      <td>74099200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:00:00.500</th>\n",
       "      <td>20230103</td>\n",
       "      <td>66240</td>\n",
       "      <td>66120</td>\n",
       "      <td>66260</td>\n",
       "      <td>99641</td>\n",
       "      <td>66160</td>\n",
       "      <td>66240</td>\n",
       "      <td>66160</td>\n",
       "      <td>99669</td>\n",
       "      <td>66210</td>\n",
       "      <td>...</td>\n",
       "      <td>66160</td>\n",
       "      <td>160</td>\n",
       "      <td>66270</td>\n",
       "      <td>2</td>\n",
       "      <td>66150</td>\n",
       "      <td>353</td>\n",
       "      <td>66280</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6952600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:00:01.000</th>\n",
       "      <td>20230103</td>\n",
       "      <td>66210</td>\n",
       "      <td>66120</td>\n",
       "      <td>66260</td>\n",
       "      <td>99641</td>\n",
       "      <td>66160</td>\n",
       "      <td>66240</td>\n",
       "      <td>66160</td>\n",
       "      <td>99682</td>\n",
       "      <td>66210</td>\n",
       "      <td>...</td>\n",
       "      <td>66160</td>\n",
       "      <td>161</td>\n",
       "      <td>66260</td>\n",
       "      <td>102</td>\n",
       "      <td>66150</td>\n",
       "      <td>353</td>\n",
       "      <td>66270</td>\n",
       "      <td>2</td>\n",
       "      <td>56.0</td>\n",
       "      <td>18541250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:00:01.500</th>\n",
       "      <td>20230103</td>\n",
       "      <td>66230</td>\n",
       "      <td>66120</td>\n",
       "      <td>66260</td>\n",
       "      <td>99641</td>\n",
       "      <td>66160</td>\n",
       "      <td>66240</td>\n",
       "      <td>66160</td>\n",
       "      <td>99681</td>\n",
       "      <td>66230</td>\n",
       "      <td>...</td>\n",
       "      <td>66200</td>\n",
       "      <td>27</td>\n",
       "      <td>66270</td>\n",
       "      <td>3</td>\n",
       "      <td>66170</td>\n",
       "      <td>64</td>\n",
       "      <td>66280</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4636150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:00:02.000</th>\n",
       "      <td>20230103</td>\n",
       "      <td>66230</td>\n",
       "      <td>66120</td>\n",
       "      <td>66260</td>\n",
       "      <td>99641</td>\n",
       "      <td>66160</td>\n",
       "      <td>66250</td>\n",
       "      <td>66160</td>\n",
       "      <td>99672</td>\n",
       "      <td>66230</td>\n",
       "      <td>...</td>\n",
       "      <td>66200</td>\n",
       "      <td>28</td>\n",
       "      <td>66300</td>\n",
       "      <td>56</td>\n",
       "      <td>66170</td>\n",
       "      <td>63</td>\n",
       "      <td>66310</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9603500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29 14:59:59.000</th>\n",
       "      <td>20231229</td>\n",
       "      <td>68920</td>\n",
       "      <td>69500</td>\n",
       "      <td>69340</td>\n",
       "      <td>155933</td>\n",
       "      <td>68970</td>\n",
       "      <td>69150</td>\n",
       "      <td>68810</td>\n",
       "      <td>144501</td>\n",
       "      <td>68910</td>\n",
       "      <td>...</td>\n",
       "      <td>68880</td>\n",
       "      <td>8</td>\n",
       "      <td>68950</td>\n",
       "      <td>4</td>\n",
       "      <td>68870</td>\n",
       "      <td>4</td>\n",
       "      <td>68960</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29 14:59:59.500</th>\n",
       "      <td>20231229</td>\n",
       "      <td>68920</td>\n",
       "      <td>69500</td>\n",
       "      <td>69340</td>\n",
       "      <td>155933</td>\n",
       "      <td>68970</td>\n",
       "      <td>69150</td>\n",
       "      <td>68810</td>\n",
       "      <td>144501</td>\n",
       "      <td>68920</td>\n",
       "      <td>...</td>\n",
       "      <td>68890</td>\n",
       "      <td>10</td>\n",
       "      <td>68960</td>\n",
       "      <td>2</td>\n",
       "      <td>68880</td>\n",
       "      <td>8</td>\n",
       "      <td>68970</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>344600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29 15:00:00.000</th>\n",
       "      <td>20231229</td>\n",
       "      <td>68920</td>\n",
       "      <td>69500</td>\n",
       "      <td>69340</td>\n",
       "      <td>155933</td>\n",
       "      <td>68970</td>\n",
       "      <td>69150</td>\n",
       "      <td>68810</td>\n",
       "      <td>144501</td>\n",
       "      <td>68920</td>\n",
       "      <td>...</td>\n",
       "      <td>68890</td>\n",
       "      <td>10</td>\n",
       "      <td>68960</td>\n",
       "      <td>2</td>\n",
       "      <td>68880</td>\n",
       "      <td>8</td>\n",
       "      <td>68970</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>344600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29 15:00:00.500</th>\n",
       "      <td>20231229</td>\n",
       "      <td>68920</td>\n",
       "      <td>69500</td>\n",
       "      <td>69340</td>\n",
       "      <td>155933</td>\n",
       "      <td>68970</td>\n",
       "      <td>69150</td>\n",
       "      <td>68810</td>\n",
       "      <td>144501</td>\n",
       "      <td>68920</td>\n",
       "      <td>...</td>\n",
       "      <td>68890</td>\n",
       "      <td>10</td>\n",
       "      <td>68960</td>\n",
       "      <td>2</td>\n",
       "      <td>68880</td>\n",
       "      <td>8</td>\n",
       "      <td>68970</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29 15:17:29.500</th>\n",
       "      <td>20231229</td>\n",
       "      <td>68920</td>\n",
       "      <td>69500</td>\n",
       "      <td>69340</td>\n",
       "      <td>155933</td>\n",
       "      <td>68970</td>\n",
       "      <td>69150</td>\n",
       "      <td>68810</td>\n",
       "      <td>144501</td>\n",
       "      <td>68920</td>\n",
       "      <td>...</td>\n",
       "      <td>68890</td>\n",
       "      <td>10</td>\n",
       "      <td>68960</td>\n",
       "      <td>2</td>\n",
       "      <td>68880</td>\n",
       "      <td>8</td>\n",
       "      <td>68970</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8182219 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         TradingDay  LastPrice  PreSettlementPrice  \\\n",
       "2023-01-03 08:59:00.500    20230103      66160               66120   \n",
       "2023-01-03 09:00:00.500    20230103      66240               66120   \n",
       "2023-01-03 09:00:01.000    20230103      66210               66120   \n",
       "2023-01-03 09:00:01.500    20230103      66230               66120   \n",
       "2023-01-03 09:00:02.000    20230103      66230               66120   \n",
       "...                             ...        ...                 ...   \n",
       "2023-12-29 14:59:59.000    20231229      68920               69500   \n",
       "2023-12-29 14:59:59.500    20231229      68920               69500   \n",
       "2023-12-29 15:00:00.000    20231229      68920               69500   \n",
       "2023-12-29 15:00:00.500    20231229      68920               69500   \n",
       "2023-12-29 15:17:29.500    20231229      68920               69500   \n",
       "\n",
       "                         PreClosePrice  PreOpenInterest  OpenPrice  \\\n",
       "2023-01-03 08:59:00.500          66260            99641      66160   \n",
       "2023-01-03 09:00:00.500          66260            99641      66160   \n",
       "2023-01-03 09:00:01.000          66260            99641      66160   \n",
       "2023-01-03 09:00:01.500          66260            99641      66160   \n",
       "2023-01-03 09:00:02.000          66260            99641      66160   \n",
       "...                                ...              ...        ...   \n",
       "2023-12-29 14:59:59.000          69340           155933      68970   \n",
       "2023-12-29 14:59:59.500          69340           155933      68970   \n",
       "2023-12-29 15:00:00.000          69340           155933      68970   \n",
       "2023-12-29 15:00:00.500          69340           155933      68970   \n",
       "2023-12-29 15:17:29.500          69340           155933      68970   \n",
       "\n",
       "                         HighestPrice  LowestPrice  OpenInterest  BidPrice1  \\\n",
       "2023-01-03 08:59:00.500         66160        66160         99658      66160   \n",
       "2023-01-03 09:00:00.500         66240        66160         99669      66210   \n",
       "2023-01-03 09:00:01.000         66240        66160         99682      66210   \n",
       "2023-01-03 09:00:01.500         66240        66160         99681      66230   \n",
       "2023-01-03 09:00:02.000         66250        66160         99672      66230   \n",
       "...                               ...          ...           ...        ...   \n",
       "2023-12-29 14:59:59.000         69150        68810        144501      68910   \n",
       "2023-12-29 14:59:59.500         69150        68810        144501      68920   \n",
       "2023-12-29 15:00:00.000         69150        68810        144501      68920   \n",
       "2023-12-29 15:00:00.500         69150        68810        144501      68920   \n",
       "2023-12-29 15:17:29.500         69150        68810        144501      68920   \n",
       "\n",
       "                         ...  BidPrice4  BidVolume4  AskPrice4  AskVolume4  \\\n",
       "2023-01-03 08:59:00.500  ...      66130         200      66260         110   \n",
       "2023-01-03 09:00:00.500  ...      66160         160      66270           2   \n",
       "2023-01-03 09:00:01.000  ...      66160         161      66260         102   \n",
       "2023-01-03 09:00:01.500  ...      66200          27      66270           3   \n",
       "2023-01-03 09:00:02.000  ...      66200          28      66300          56   \n",
       "...                      ...        ...         ...        ...         ...   \n",
       "2023-12-29 14:59:59.000  ...      68880           8      68950           4   \n",
       "2023-12-29 14:59:59.500  ...      68890          10      68960           2   \n",
       "2023-12-29 15:00:00.000  ...      68890          10      68960           2   \n",
       "2023-12-29 15:00:00.500  ...      68890          10      68960           2   \n",
       "2023-12-29 15:17:29.500  ...      68890          10      68960           2   \n",
       "\n",
       "                         BidPrice5  BidVolume5  AskPrice5  AskVolume5  \\\n",
       "2023-01-03 08:59:00.500      66120          13      66270          30   \n",
       "2023-01-03 09:00:00.500      66150         353      66280           1   \n",
       "2023-01-03 09:00:01.000      66150         353      66270           2   \n",
       "2023-01-03 09:00:01.500      66170          64      66280           1   \n",
       "2023-01-03 09:00:02.000      66170          63      66310           1   \n",
       "...                            ...         ...        ...         ...   \n",
       "2023-12-29 14:59:59.000      68870           4      68960           2   \n",
       "2023-12-29 14:59:59.500      68880           8      68970          11   \n",
       "2023-12-29 15:00:00.000      68880           8      68970          11   \n",
       "2023-12-29 15:00:00.500      68880           8      68970          11   \n",
       "2023-12-29 15:17:29.500      68880           8      68970          11   \n",
       "\n",
       "                         delta_Volume  delta_Turnover  \n",
       "2023-01-03 08:59:00.500         224.0      74099200.0  \n",
       "2023-01-03 09:00:00.500          21.0       6952600.0  \n",
       "2023-01-03 09:00:01.000          56.0      18541250.0  \n",
       "2023-01-03 09:00:01.500          14.0       4636150.0  \n",
       "2023-01-03 09:00:02.000          29.0       9603500.0  \n",
       "...                               ...             ...  \n",
       "2023-12-29 14:59:59.000           8.0       2756650.0  \n",
       "2023-12-29 14:59:59.500           1.0        344600.0  \n",
       "2023-12-29 15:00:00.000           1.0        344600.0  \n",
       "2023-12-29 15:00:00.500           0.0             0.0  \n",
       "2023-12-29 15:17:29.500           0.0             0.0  \n",
       "\n",
       "[8182219 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/cu.parquet'\n",
    "\n",
    "data = pd.read_parquet(path)\n",
    "data.replace(-121, np.nan, inplace=True)\n",
    "print(set(data['TradingDay'].values))\n",
    "data = data[['TradingDay', 'LastPrice', 'PreSettlementPrice', 'PreClosePrice',\n",
    "             'PreOpenInterest', 'OpenPrice', 'HighestPrice', 'LowestPrice', 'OpenInterest',\n",
    "             'BidPrice1', 'BidVolume1', 'AskPrice1', 'AskVolume1',\n",
    "             'BidPrice2', 'BidVolume2', 'AskPrice2', 'AskVolume2',\n",
    "             'BidPrice3', 'BidVolume3', 'AskPrice3', 'AskVolume3',\n",
    "             'BidPrice4', 'BidVolume4', 'AskPrice4', 'AskVolume4',\n",
    "             'BidPrice5', 'BidVolume5', 'AskPrice5', 'AskVolume5',\n",
    "             'delta_Volume', 'delta_Turnover']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['BidPrice1', 'BidVolume1', 'AskPrice1', 'AskVolume1',\n",
    "             'BidPrice2', 'BidVolume2', 'AskPrice2', 'AskVolume2',\n",
    "             'BidPrice3', 'BidVolume3', 'AskPrice3', 'AskVolume3',\n",
    "             'BidPrice4', 'BidVolume4', 'AskPrice4', 'AskVolume4',\n",
    "             'BidPrice5', 'BidVolume5', 'AskPrice5', 'AskVolume5',\n",
    "             'delta_Volume', 'delta_Turnover']\n",
    "\n",
    "# feature_cols = ['BidPrice1', 'BidVolume1', 'AskPrice1', 'AskVolume1',\n",
    "#              'BidPrice2', 'BidVolume2', 'AskPrice2', 'AskVolume2',\n",
    "#              'BidPrice3', 'BidVolume3', 'AskPrice3', 'AskVolume3',\n",
    "#              'BidPrice4', 'BidVolume4', 'AskPrice4', 'AskVolume4',\n",
    "#              'BidPrice5', 'BidVolume5', 'AskPrice5', 'AskVolume5']\n",
    "\n",
    "price_cols = ['BidPrice1', 'AskPrice1', 'BidPrice2',\n",
    "              'AskPrice2', 'BidPrice3', 'AskPrice3',\n",
    "              'BidPrice4', 'AskPrice4', 'BidPrice5', 'AskPrice5']\n",
    "\n",
    "vol_cols = ['BidVolume1', 'AskVolume1', 'BidVolume2', 'AskVolume2',\n",
    "            'BidVolume3', 'AskVolume3', 'BidVolume4', 'AskVolume4',\n",
    "            'BidVolume5', 'AskVolume5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(raw_data, ret_window = 120):\n",
    "    raw_data['midprice'] = (raw_data['BidPrice1']+raw_data['AskPrice1'])/2\n",
    "    raw_data['fut_midprice'] = raw_data['midprice'].shift(-ret_window)\n",
    "    raw_data['label'] = 100*(raw_data['fut_midprice']-raw_data['midprice'])/raw_data['midprice']\n",
    "\n",
    "    raw_data['midvolume'] = (raw_data['BidVolume1']+raw_data['AskVolume1'])/2\n",
    "\n",
    "    # price_rolling_mean = raw_data['midprice'].rolling(window=10).mean()\n",
    "    # volume_rolling_mean = raw_data['midvolume'].rolling(window=10).mean()\n",
    "\n",
    "    # raw_data['rolling_midprice'] = price_rolling_mean\n",
    "    # raw_data['rolling_midvolume'] = volume_rolling_mean\n",
    "\n",
    "    raw_data.drop(raw_data.tail(ret_window).index, inplace=True)\n",
    "    raw_data.drop(raw_data.head(10).index, inplace=True)\n",
    "\n",
    "    label = raw_data[['label']].values\n",
    "\n",
    "    # raw_data[vol_cols] = raw_data[vol_cols].div(raw_data['rolling_midvolume'], axis=0)\n",
    "    # raw_data[price_cols] = raw_data[price_cols].div(raw_data['rolling_midprice'], axis=0)-1\n",
    "    # raw_data[price_cols] = 10000*raw_data[price_cols]\n",
    "    # raw_data[vol_cols] = np.log(raw_data[vol_cols])\n",
    "    raw_data = raw_data[feature_cols].values\n",
    "\n",
    "    return raw_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6.621000e+04, 2.290000e+02, 6.622000e+04, ..., 1.020000e+02,\n",
       "         4.600000e+01, 1.522470e+07],\n",
       "        [6.621000e+04, 1.990000e+02, 6.623000e+04, ..., 3.000000e+00,\n",
       "         5.600000e+01, 1.853955e+07],\n",
       "        [6.622000e+04, 4.000000e+00, 6.623000e+04, ..., 2.000000e+00,\n",
       "         3.800000e+01, 1.258075e+07],\n",
       "        ...,\n",
       "        [6.890000e+04, 5.500000e+01, 6.891000e+04, ..., 4.400000e+01,\n",
       "         0.000000e+00, 0.000000e+00],\n",
       "        [6.890000e+04, 5.500000e+01, 6.891000e+04, ..., 4.400000e+01,\n",
       "         0.000000e+00, 0.000000e+00],\n",
       "        [6.890000e+04, 5.000000e+01, 6.891000e+04, ..., 4.400000e+01,\n",
       "         0.000000e+00, 0.000000e+00]]),\n",
       " array([[-0.04530695],\n",
       "        [-0.05285412],\n",
       "        [-0.06040015],\n",
       "        ...,\n",
       "        [ 0.02902547],\n",
       "        [ 0.02902547],\n",
       "        [ 0.02902547]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data_preprocess(data)\n",
    "del data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.000000e+01, 1.590000e+02, 2.100000e+01, 9.900000e+01,\n",
       "        1.540000e+02, 1.010000e+02, 1.570000e+02, 3.420000e+02,\n",
       "        7.620000e+02, 1.379000e+03, 5.132000e+03, 4.062100e+04,\n",
       "        7.732389e+06, 3.898390e+05, 5.676000e+03, 2.859000e+03,\n",
       "        1.150000e+03, 5.320000e+02, 3.980000e+02, 2.590000e+02]),\n",
       " array([-2.32287191, -2.13952693, -1.95618195, -1.77283697, -1.58949198,\n",
       "        -1.406147  , -1.22280202, -1.03945703, -0.85611205, -0.67276707,\n",
       "        -0.48942208, -0.3060771 , -0.12273212,  0.06061287,  0.24395785,\n",
       "         0.42730283,  0.61064782,  0.7939928 ,  0.97733778,  1.16068276,\n",
       "         1.34402775]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dataset[1], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, period=600, small_period=1):\n",
    "        self.data = data\n",
    "        self.period = period\n",
    "        self.processed_data_x = []\n",
    "        self.processed_data_y = []\n",
    "\n",
    "        for i in range(period, len(data[0]), small_period):\n",
    "            self.processed_data_x.append(data[0][i-period:i, :])\n",
    "            self.processed_data_y.append(data[1][i-1])\n",
    "\n",
    "        self.processed_data_x = np.stack(self.processed_data_x, axis=0)\n",
    "        self.processed_data_y = np.stack(self.processed_data_y, axis=0)\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.processed_data_y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data_x[idx, :,:], self.processed_data_y[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327254\n",
      "40881\n",
      "40881\n"
     ]
    }
   ],
   "source": [
    "train_data = MyDataset((dataset[0][:int(0.8*len(dataset[0])), :], dataset[1][:int(0.8*len(dataset[0])), :]), 600, 20)\n",
    "val_data = MyDataset((dataset[0][int(0.8*len(dataset[0])): int(0.9*len(dataset[0])), :], dataset[1][int(0.8*len(dataset[0])): int(0.9*len(dataset[0])), :]), 600, 20)\n",
    "test_data = MyDataset((dataset[0][int(0.9*len(dataset[0])):, :], dataset[1][int(0.9*len(dataset[0])):, :]), 600, 20)\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_data = DataLoader (val_data, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_data = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, nhead):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers, nhead):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        x = self.transformer_decoder(x, memory)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class SimLOB(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, nhead):\n",
    "        super(SimLOB, self).__init__()\n",
    "        self.encoder = TransformerEncoder(input_dim, hidden_dim, num_layers, nhead)\n",
    "        self.decoder = TransformerDecoder(hidden_dim, output_dim, num_layers, nhead)\n",
    "\n",
    "    def forward(self, x):\n",
    "        memory = self.encoder(x)\n",
    "        output = self.decoder(x, memory)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "was expecting embedding dimension of 64, but got 40",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m SimLOB(input_dim, hidden_dim, output_dim, num_layers, nhead)\n\u001b[1;32m      8\u001b[0m sample_lob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m10\u001b[39m, input_dim)  \u001b[38;5;66;03m# (sequence_length, batch_size, input_dim)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m model(sample_lob)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[15], line 33\u001b[0m, in \u001b[0;36mSimLOB.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     32\u001b[0m     memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m---> 33\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x, memory)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, x, memory)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, memory):\n\u001b[0;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_decoder(x, memory)\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(x)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:369\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 369\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(output, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[1;32m    370\u001b[0m                  memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    371\u001b[0m                  tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    372\u001b[0m                  memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:716\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    714\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[1;32m    717\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m    718\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:725\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    724\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, x, x,\n\u001b[1;32m    726\u001b[0m                        attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m    727\u001b[0m                        key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    728\u001b[0m                        is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    729\u001b[0m                        need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1192\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1203\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1206\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1207\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1208\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[1;32m   1209\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1210\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1211\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1212\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1213\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1214\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1215\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:5204\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5199\u001b[0m         \u001b[38;5;66;03m# We have the attn_mask, and use that to merge kpm into it.\u001b[39;00m\n\u001b[1;32m   5200\u001b[0m         \u001b[38;5;66;03m# Turn off use of is_causal hint, as the merged mask is no\u001b[39;00m\n\u001b[1;32m   5201\u001b[0m         \u001b[38;5;66;03m# longer causal.\u001b[39;00m\n\u001b[1;32m   5202\u001b[0m         is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 5204\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m embed_dim \u001b[38;5;241m==\u001b[39m embed_dim_to_check, \\\n\u001b[1;32m   5205\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas expecting embedding dimension of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_dim, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m   5207\u001b[0m     \u001b[38;5;66;03m# embed_dim can be a tensor when JIT tracing\u001b[39;00m\n\u001b[1;32m   5208\u001b[0m     head_dim \u001b[38;5;241m=\u001b[39m embed_dim\u001b[38;5;241m.\u001b[39mdiv(num_heads, rounding_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrunc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: was expecting embedding dimension of 64, but got 40"
     ]
    }
   ],
   "source": [
    "input_dim = 40\n",
    "hidden_dim = 64\n",
    "output_dim = 40\n",
    "num_layers = 2\n",
    "nhead = 4\n",
    "\n",
    "model = SimLOB(input_dim, hidden_dim, output_dim, num_layers, nhead)\n",
    "sample_lob = torch.randn(100, 10, input_dim)  # (sequence_length, batch_size, input_dim)\n",
    "output = model(sample_lob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, loss function, and optimizer\n",
    "model = SimLOB(input_dim, hidden_dim, output_dim, num_layers, nhead)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "train_model(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss / len(test_loader)}\")\n",
    "\n",
    "test_model(model, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
